+++
date = 2020-01-01T00:00:00  # Schedule page publish date.

title = "IEOR 265 - Learning  Optimization - Spring 2020"
time_start = 2030-06-01T13:00:00
time_end = 2030-06-01T15:00:00
abstract = ""
abstract_short = ""
event = ""
event_url = ""
location = "University of California Berkeley"

# Is this a selected talk? (true/false)
selected = false

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter the filename (excluding '.md') of your project file in `content/project/`.

# Links (optional).
url_pdf = ""
url_slides = ""
url_video = ""
url_code = ""

# Does the content use math formatting?
math = true

# Does the content use source code highlighting?
highlight = true

# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
[header]
image = ""
caption = ""

+++

Course Description:

This course will cover topics related to the interplay between optimization and statistical learning. The rst part of the course will cover the fundamentals, methods, and algorithms for dynamic programming, and optimal control. We will study approximate dynamic programming and the formulation and numerical implementation of several different algorithms and reinforcement learning methods. In addition we will study learning-based model predictive control (LBMPC), which is a method for robust adaptive optimization that can use machine learning to provide the adaptation online. The second part of the course will deal with inverse decision-making problems, which are problems where an agent's decisions are observed and used to infer properties about the agent, such as preferences, utility functions, etc.

Course {{% staticref "files/Syllabus.pdf" "newtab" %}}Syllabus - Spring 2020{{% /staticref %}}

Course Material: 

Lecture 1: Introduction to Dynamic Programming {{% staticref "files/Note1.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_1.pdf" "newtab" %}}chess match example{{% /staticref %}}

Lecture 2: Deterministic Dynamic Programming {{% staticref "files/Note2.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_2.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 3: Hidden Markov Models {{% staticref "files/Note3.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_3.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 4: Stochastic DP and LQR model {{% staticref "files/Note4.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_4.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 5: Approximate DP and Reinforcement Learning {{% staticref "files/Lecture_slides_5.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 6: Approximation in Value Space {{% staticref "files/Note6.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_6.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 7: Approximation Architectures and (Deep) Neural Networks {{% staticref "files/Note7.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_7.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 8: Value Iteration and the DQN Algorithm {{% staticref "files/Note8.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_8.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 9: DQN Algorithm implementation {{% staticref "files/Lecture_slides_9.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 10: Policy Iteration {{% staticref "files/Note10.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_10.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 11: Approximate PI, Critic Algorithm and Policy Gradient {{% staticref "files/Note11.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_11.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 12: The Actor-Critic Algorithm {{% staticref "files/Note12.pdf" "newtab" %}}lecture notes{{% /staticref %}} - {{% staticref "files/Lecture_slides_12.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 13: AlphaGo Implementation and Analysis {{% staticref "files/Lecture_slides_13.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 14: Monte-Carlo Tree Search {{% staticref "files/Lecture_slides_14.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 15: Model Predictive Control (MPC) {{% staticref "files/Lecture_slides_15.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 16: Establishing MPC Properties {{% staticref "files/Lecture_slides_16.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 17: Invariant Sets and Polyhedral Operations {{% staticref "files/Lecture_slides_17.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 18: Robust Model Predictive Control {{% staticref "files/Lecture_slides_18.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 19: Learning-based Model Predictive Control (LBMPC) {{% staticref "files/Lecture_slides_19.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 20: Recap of Approximate DP and Reinforcement Learning {{% staticref "files/Lecture_slides_20.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 21: Inverse Decision Making Problems {{% staticref "files/Lecture_slides_21.pdf" "newtab" %}}lecture slides{{% /staticref %}}

Lecture 22: Inverse Optimization and Variational Inequalities {{% staticref "files/Lecture_slides_22.pdf" "newtab" %}}lecture slides{{% /staticref %}}























